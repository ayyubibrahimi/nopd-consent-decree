{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/305 [==============================] - 0s 275us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Function to read the CSV file\n",
    "def read_tbl(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# Load the neural network model and tokenizer\n",
    "nn_model = load_model('../../train/data/output/gist_classification_model.h5')\n",
    "with open('../../train/data/output/tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# Load the logistic regression model\n",
    "with open('../../train/data/output/gist_classification_lr_model.pkl', 'rb') as handle:\n",
    "    lr_model = pickle.load(handle)\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Read the input CSV file\n",
    "df = read_tbl(\"../../ner/data/output/combined_output.csv\")\n",
    "\n",
    "# Preprocess texts for neural network model\n",
    "texts = df['gist'].astype(str).tolist()\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "maxlen = 100\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Predict using the neural network model\n",
    "nn_predictions = nn_model.predict(data)\n",
    "nn_predicted_labels = (nn_predictions > 0.5).astype(\"int32\")\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "df['nn_relevant'] = nn_predicted_labels\n",
    "\n",
    "# Save the DataFrame with neural network predictions\n",
    "df.to_csv(\"../data/output/output_nn.csv\", index=False)\n",
    "\n",
    "# Preprocess texts for logistic regression model\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in texts:\n",
    "    encoded_dict = bert_tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = np.concatenate(input_ids, axis=0)\n",
    "attention_masks = np.concatenate(attention_masks, axis=0)\n",
    "\n",
    "# Generate BERT embeddings\n",
    "bert_outputs = bert_model(input_ids, attention_mask=attention_masks)\n",
    "X = bert_outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "# Use the pre-trained logistic regression model for predictions\n",
    "lr_predicted_labels = lr_model.predict(X)\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "df['lr_relevant'] = lr_predicted_labels\n",
    "\n",
    "# Save the DataFrame with logistic regression predictions\n",
    "df.to_csv(\"../data/output/output_lr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer = model.get_layer('embedding_2')\n",
    "# embeddings = embedding_layer.get_weights()[0]\n",
    "# print(\"Embedding shape:\", embeddings.shape)\n",
    "# print(\"Sample embeddings:\")\n",
    "# print(embeddings[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
